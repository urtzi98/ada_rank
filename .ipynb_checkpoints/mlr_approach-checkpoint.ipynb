{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from operator import itemgetter\n",
    "from gensim import similarities\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf model\n",
    "This model has been used in order to orientate us when making the labelling of the dataset. Specially for the long_common_name feature the model has been very usefull for ranking each document. For the adaRank model it is not necessary to run all this functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()# object allowing a stemmer to return words in root form\n",
    "    tokens = wordpunct_tokenize(doc) # tokenize docs \n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    "    final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(docs):\n",
    "    pdocs = [preprocess_document(doc) for doc in docs]\n",
    "    dictionary = corpora.Dictionary(pdocs)\n",
    "    dictionary.save('/tmp/vsm.dict')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bag of words-based representation for each long name in the list\n",
    "def docs2bows(corpus, dictionary):\n",
    "    docs = [preprocess_document(d) for d in corpus]\n",
    "    vectors = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize('/tmp/vsm_docs.mm', vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weghted counterparts\n",
    "def create_TF_IDF_model(corpus):\n",
    "    dictionary = create_dictionary(corpus)\n",
    "    docs2bows(corpus, dictionary)\n",
    "    loaded_corpus = corpora.MmCorpus('/tmp/vsm_docs.mm')\n",
    "    tfidf = models.TfidfModel(loaded_corpus)\n",
    "    return tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_query(corpus, q, filename='/tmp/vsm_docs.mm'):\n",
    "    sorted_scores = []\n",
    "    sorted_docs = []\n",
    "    tfidf, dictionary = create_TF_IDF_model(corpus)\n",
    "    loaded_corpus = corpora.MmCorpus(filename)\n",
    "    index = similarities.MatrixSimilarity(loaded_corpus, num_features=len(dictionary))\n",
    "    pq = preprocess_document(q)\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = tfidf[vq]\n",
    "    sim = index[qtfidf]\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "    for doc, score in ranking:\n",
    "        if score > 0.0:\n",
    "            sorted_scores.append(score)\n",
    "            sorted_docs.append(corpus[doc])\n",
    "            print(\"[ Score = \" + \"%f\" % score + \" ] \" + corpus[doc]);\n",
    "    return sorted_scores, sorted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to try how the tf-idf model works on the long_common_name feature for the three csv pages:\n",
    "# (text_query1, text_query2, text_query3) the variables must be changed before the execution\n",
    "print(text_query3)\n",
    "sorted_scores, sorted_docs = launch_query(data[text_query3][long_name], text_query3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adaRank model\n",
    "We read the labelled data, all the data is normalized and inverted for a correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the three pages sepparately\n",
    "path = \"loinc_dataset-v2.xlsx\"\n",
    "document = xlrd.open_workbook(path)\n",
    "query1 = document.sheet_by_index(0)\n",
    "query2 = document.sheet_by_index(1)\n",
    "query3 = document.sheet_by_index(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 6)\n"
     ]
    }
   ],
   "source": [
    "# Glucose in blood is taken into account\n",
    "rows_query1 = query1.nrows\n",
    "# String of the first query that is glucose in blood\n",
    "part_list = query1.cell_value(0,0).split(\":\")[1].split(\" \")[1:4]\n",
    "text_query1 = \" \".join(part_list).lower()\n",
    "loinc_num = []\n",
    "long_name = []\n",
    "component = []\n",
    "system = []\n",
    "properti = []\n",
    "qid = []\n",
    "label_ranking = []\n",
    "for i in range(3, rows_query1):\n",
    "    loinc_num.append(query1.cell_value(i,0))\n",
    "    long_name.append(query1.cell_value(i,1))\n",
    "    component.append(query1.cell_value(i,2))\n",
    "    system.append(query1.cell_value(i,3))\n",
    "    properti.append(query1.cell_value(i,4))\n",
    "    qid.append(query1.cell_value(i,5))\n",
    "    label_ranking.append(query1.cell_value(i,6))\n",
    "max_long = max(long_name)\n",
    "long_name = np.array([1-(nam/max_long) for nam in long_name])\n",
    "max_component = max(component)\n",
    "component = np.array([1-(comp/max_component) for comp in component])\n",
    "max_system = max(system)\n",
    "system = np.array([1-(sys/max_system) for sys in system])\n",
    "max_properti = max(properti)\n",
    "properti = np.array([1-(prop/max_properti) for prop in properti])\n",
    "qid = np.array(qid)\n",
    "max_label = max(label_ranking)\n",
    "label_ranking = np.array([1-(lab/max_label) for lab in label_ranking])\n",
    "row = label_ranking.shape\n",
    "data1 = np.zeros((row[0],6))\n",
    "data1[:,0] = long_name\n",
    "data1[:,1] = component\n",
    "data1[:,2] = system\n",
    "data1[:,3] = properti\n",
    "data1[:,4] = qid\n",
    "data1[:,5] = label_ranking\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 6)\n"
     ]
    }
   ],
   "source": [
    "# bilirubin in plasma\n",
    "rows_query2 = query2.nrows\n",
    "part_list = query2.cell_value(0,0).split(\":\")[1].split(\" \")[1:4]\n",
    "text_query2 = \" \".join(part_list).lower()\n",
    "loinc_num = []\n",
    "long_name = []\n",
    "component = []\n",
    "system = []\n",
    "properti = []\n",
    "qid = []\n",
    "label_ranking = []\n",
    "for i in range(3, rows_query2):\n",
    "    loinc_num.append(query2.cell_value(i,0))\n",
    "    long_name.append(query2.cell_value(i,1))\n",
    "    component.append(query2.cell_value(i,2))\n",
    "    system.append(query2.cell_value(i,3))\n",
    "    properti.append(query2.cell_value(i,4))\n",
    "    qid.append(query2.cell_value(i,5))\n",
    "    label_ranking.append(query2.cell_value(i,6))\n",
    "max_long = max(long_name)\n",
    "long_name = np.array([1-(nam/max_long) for nam in long_name])\n",
    "max_component = max(component)\n",
    "component = np.array([1-(comp/max_component) for comp in component])\n",
    "max_system = max(system)\n",
    "system = np.array([1-(sys/max_system) for sys in system])\n",
    "max_properti = max(properti)\n",
    "properti = np.array([1-(prop/max_properti) for prop in properti])\n",
    "qid = np.array(qid)\n",
    "max_label = max(label_ranking)\n",
    "label_ranking = np.array([1-(lab/max_label) for lab in label_ranking])\n",
    "row = label_ranking.shape\n",
    "data2 = np.zeros((row[0],6))\n",
    "data2[:,0] = long_name\n",
    "data2[:,1] = component\n",
    "data2[:,2] = system\n",
    "data2[:,3] = properti\n",
    "data2[:,4] = qid\n",
    "data2[:,5] = label_ranking\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 6)\n"
     ]
    }
   ],
   "source": [
    "# White blood cells count\n",
    "rows_query3 = query3.nrows\n",
    "part_list = query3.cell_value(0,0).split(\":\")[1].split(\" \")[1:4]\n",
    "text_query3 = \" \".join(part_list).lower()\n",
    "loinc_num = []\n",
    "long_name = []\n",
    "component = []\n",
    "system = []\n",
    "properti = []\n",
    "qid = []\n",
    "label_ranking = []\n",
    "for i in range(3, rows_query3):\n",
    "    loinc_num.append(query3.cell_value(i,0))\n",
    "    long_name.append(query3.cell_value(i,1))\n",
    "    component.append(query3.cell_value(i,2))\n",
    "    system.append(query3.cell_value(i,3))\n",
    "    properti.append(query3.cell_value(i,4))\n",
    "    qid.append(query3.cell_value(i,5))\n",
    "    label_ranking.append(query3.cell_value(i,6))\n",
    "max_long = max(long_name)\n",
    "long_name = np.array([1-(nam/max_long) for nam in long_name])\n",
    "max_component = max(component)\n",
    "component = np.array([1-(comp/max_component) for comp in component])\n",
    "max_system = max(system)\n",
    "system = np.array([1-(sys/max_system) for sys in system])\n",
    "max_properti = max(properti)\n",
    "properti = np.array([1-(prop/max_properti) for prop in properti])\n",
    "qid = np.array(qid)\n",
    "max_label = max(label_ranking)\n",
    "label_ranking = np.array([1-(lab/max_label) for lab in label_ranking])\n",
    "row = label_ranking.shape\n",
    "data3 = np.zeros((row[0],6))\n",
    "data3[:,0] = long_name\n",
    "data3[:,1] = component\n",
    "data3[:,2] = system\n",
    "data3[:,3] = properti\n",
    "data3[:,4] = qid\n",
    "data3[:,5] = label_ranking\n",
    "print(data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 6)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the three arrays\n",
    "final_data = np.concatenate((data1, data2, data3), axis=0)\n",
    "print(final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adarank import AdaRank\n",
    "from metric import NDCGScorer_qid\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = final_data[:,:4]\n",
    "y = final_data[:,5].ravel()\n",
    "qid = final_data[:,4].ravel()\n",
    "X_train, X_test, y_train, y_test, qid_train, qid_test = train_test_split(X, y, qid, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the adaRank model: 0.9258429746159355\n",
      "Accuracy percentage for the adaRank model: 92.58429746159355 %\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_valid, y_train, y_valid, qid_train, qid_valid = train_test_split(X_train, y_train, qid_train, test_size=0.2, random_state=42)\n",
    "model = AdaRank(scorer=NDCGScorer_qid(K=5))\n",
    "model.fit(X, y, qid, X, y, qid)\n",
    "pred = model.predict(X_test)\n",
    "#print(pred)\n",
    "accuracy = NDCGScorer_qid(K=5)(y_test,pred,qid_test).mean()\n",
    "print(\"Accuracy for the adaRank model:\",accuracy)\n",
    "print(\"Accuracy percentage for the adaRank model:\",accuracy*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
