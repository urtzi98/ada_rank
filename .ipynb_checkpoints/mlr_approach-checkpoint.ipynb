{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from operator import itemgetter\n",
    "from gensim import similarities\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf model\n",
    "This model has been used in order to orientate us when making the labelling of the dataset. Specially for the long_common_name feature the model has been very usefull for ranking each document. For the adaRank model it is not necessary to run all this functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()# object allowing a stemmer to return words in root form\n",
    "    tokens = wordpunct_tokenize(doc) # tokenize docs \n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    "    final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(docs):\n",
    "    pdocs = [preprocess_document(doc) for doc in docs]\n",
    "    dictionary = corpora.Dictionary(pdocs)\n",
    "    dictionary.save('/tmp/vsm.dict')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bag of words-based representation for each long name in the list\n",
    "def docs2bows(corpus, dictionary):\n",
    "    docs = [preprocess_document(d) for d in corpus]\n",
    "    vectors = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize('/tmp/vsm_docs.mm', vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weghted counterparts\n",
    "def create_TF_IDF_model(corpus):\n",
    "    dictionary = create_dictionary(corpus)\n",
    "    docs2bows(corpus, dictionary)\n",
    "    loaded_corpus = corpora.MmCorpus('/tmp/vsm_docs.mm')\n",
    "    tfidf = models.TfidfModel(loaded_corpus)\n",
    "    return tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_query(corpus, q, filename='/tmp/vsm_docs.mm'):\n",
    "    sorted_scores = []\n",
    "    sorted_docs = []\n",
    "    tfidf, dictionary = create_TF_IDF_model(corpus)\n",
    "    loaded_corpus = corpora.MmCorpus(filename)\n",
    "    index = similarities.MatrixSimilarity(loaded_corpus, num_features=len(dictionary))\n",
    "    pq = preprocess_document(q)\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = tfidf[vq]\n",
    "    sim = index[qtfidf]\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "    for doc, score in ranking:\n",
    "        if score > 0.0:\n",
    "            sorted_scores.append(score)\n",
    "            sorted_docs.append(corpus[doc])\n",
    "            print(\"[ Score = \" + \"%f\" % score + \" ] \" + corpus[doc]);\n",
    "    return sorted_scores, sorted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to try how the tf-idf model works on the long_common_name feature for the three csv pages:\n",
    "# (text_query1, text_query2, text_query3) the variables must be changed before the execution\n",
    "print(text_query3)\n",
    "sorted_scores, sorted_docs = launch_query(data[text_query3][long_name], text_query3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaRank model\n",
    "We read the labelled data, all the data is normalized and inverted to get correct values for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the three pages sepparately\n",
    "path = \"loinc_dataset-v2.xlsx\"\n",
    "document = xlrd.open_workbook(path)\n",
    "query1 = document.sheet_by_index(0)\n",
    "query2 = document.sheet_by_index(1)\n",
    "query3 = document.sheet_by_index(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 6)\n"
     ]
    }
   ],
   "source": [
    "# Glucose in blood is taken into account\n",
    "rows_query1 = query1.nrows\n",
    "# String of the first query that is glucose in blood\n",
    "part_list = query1.cell_value(0,0).split(\":\")[1].split(\" \")[1:4]\n",
    "text_query1 = \" \".join(part_list).lower()\n",
    "loinc_num = []\n",
    "long_name = []\n",
    "component = []\n",
    "system = []\n",
    "properti = []\n",
    "qid = []\n",
    "label_ranking = []\n",
    "for i in range(3, rows_query1):\n",
    "    loinc_num.append(query1.cell_value(i,0))\n",
    "    long_name.append(query1.cell_value(i,1))\n",
    "    component.append(query1.cell_value(i,2))\n",
    "    system.append(query1.cell_value(i,3))\n",
    "    properti.append(query1.cell_value(i,4))\n",
    "    qid.append(query1.cell_value(i,5))\n",
    "    label_ranking.append(query1.cell_value(i,6))\n",
    "max_long = max(long_name)\n",
    "long_name = np.array([1-(nam/max_long) for nam in long_name])\n",
    "max_component = max(component)\n",
    "component = np.array([1-(comp/max_component) for comp in component])\n",
    "max_system = max(system)\n",
    "system = np.array([1-(sys/max_system) for sys in system])\n",
    "max_properti = max(properti)\n",
    "properti = np.array([1-(prop/max_properti) for prop in properti])\n",
    "qid = np.array(qid)\n",
    "max_label = max(label_ranking)\n",
    "label_ranking = np.array([1-(lab/max_label) for lab in label_ranking])\n",
    "row = label_ranking.shape\n",
    "data1 = np.zeros((row[0],6))\n",
    "data1[:,0] = long_name\n",
    "data1[:,1] = component\n",
    "data1[:,2] = system\n",
    "data1[:,3] = properti\n",
    "data1[:,4] = qid\n",
    "data1[:,5] = label_ranking\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 6)\n"
     ]
    }
   ],
   "source": [
    "# bilirubin in plasma\n",
    "rows_query2 = query2.nrows\n",
    "part_list = query2.cell_value(0,0).split(\":\")[1].split(\" \")[1:4]\n",
    "text_query2 = \" \".join(part_list).lower()\n",
    "loinc_num = []\n",
    "long_name = []\n",
    "component = []\n",
    "system = []\n",
    "properti = []\n",
    "qid = []\n",
    "label_ranking = []\n",
    "for i in range(3, rows_query2):\n",
    "    loinc_num.append(query2.cell_value(i,0))\n",
    "    long_name.append(query2.cell_value(i,1))\n",
    "    component.append(query2.cell_value(i,2))\n",
    "    system.append(query2.cell_value(i,3))\n",
    "    properti.append(query2.cell_value(i,4))\n",
    "    qid.append(query2.cell_value(i,5))\n",
    "    label_ranking.append(query2.cell_value(i,6))\n",
    "max_long = max(long_name)\n",
    "long_name = np.array([1-(nam/max_long) for nam in long_name])\n",
    "max_component = max(component)\n",
    "component = np.array([1-(comp/max_component) for comp in component])\n",
    "max_system = max(system)\n",
    "system = np.array([1-(sys/max_system) for sys in system])\n",
    "max_properti = max(properti)\n",
    "properti = np.array([1-(prop/max_properti) for prop in properti])\n",
    "qid = np.array(qid)\n",
    "max_label = max(label_ranking)\n",
    "label_ranking = np.array([1-(lab/max_label) for lab in label_ranking])\n",
    "row = label_ranking.shape\n",
    "data2 = np.zeros((row[0],6))\n",
    "data2[:,0] = long_name\n",
    "data2[:,1] = component\n",
    "data2[:,2] = system\n",
    "data2[:,3] = properti\n",
    "data2[:,4] = qid\n",
    "data2[:,5] = label_ranking\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 6)\n"
     ]
    }
   ],
   "source": [
    "# White blood cells count\n",
    "rows_query3 = query3.nrows\n",
    "part_list = query3.cell_value(0,0).split(\":\")[1].split(\" \")[1:4]\n",
    "text_query3 = \" \".join(part_list).lower()\n",
    "loinc_num = []\n",
    "long_name = []\n",
    "component = []\n",
    "system = []\n",
    "properti = []\n",
    "qid = []\n",
    "label_ranking = []\n",
    "for i in range(3, rows_query3):\n",
    "    loinc_num.append(query3.cell_value(i,0))\n",
    "    long_name.append(query3.cell_value(i,1))\n",
    "    component.append(query3.cell_value(i,2))\n",
    "    system.append(query3.cell_value(i,3))\n",
    "    properti.append(query3.cell_value(i,4))\n",
    "    qid.append(query3.cell_value(i,5))\n",
    "    label_ranking.append(query3.cell_value(i,6))\n",
    "max_long = max(long_name)\n",
    "long_name = np.array([1-(nam/max_long) for nam in long_name])\n",
    "max_component = max(component)\n",
    "component = np.array([1-(comp/max_component) for comp in component])\n",
    "max_system = max(system)\n",
    "system = np.array([1-(sys/max_system) for sys in system])\n",
    "max_properti = max(properti)\n",
    "properti = np.array([1-(prop/max_properti) for prop in properti])\n",
    "qid = np.array(qid)\n",
    "max_label = max(label_ranking)\n",
    "label_ranking = np.array([1-(lab/max_label) for lab in label_ranking])\n",
    "row = label_ranking.shape\n",
    "data3 = np.zeros((row[0],6))\n",
    "data3[:,0] = long_name\n",
    "data3[:,1] = component\n",
    "data3[:,2] = system\n",
    "data3[:,3] = properti\n",
    "data3[:,4] = qid\n",
    "data3[:,5] = label_ranking\n",
    "print(data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 6)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the three arrays\n",
    "final_data = np.concatenate((data1, data2, data3), axis=0)\n",
    "print(final_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load varaibles X, y, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adarank import AdaRank\n",
    "from metric import NDCGScorer_qid\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = final_data[:,:4]\n",
    "y = final_data[:,5].ravel()\n",
    "qid = final_data[:,4].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set division function\n",
    "Functions that divide a set into two subsets with size 80 % and 20 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(X, y, qid, query_id):\n",
    "    indx = np.where(qid==query_id)\n",
    "    num_samples = X[indx].shape[0]\n",
    "    X_train = X[indx][:round(num_samples * 0.8),:]\n",
    "    X_test = X[indx][round(num_samples * 0.8):,:]\n",
    "    y_train = y[indx][:round(num_samples * 0.8)]\n",
    "    y_test = y[indx][round(num_samples * 0.8):]\n",
    "    qid_train = qid[indx][:round(num_samples * 0.8)]\n",
    "    qid_test = qid[indx][round(num_samples * 0.8):]\n",
    "    return X_train, X_test, y_train, y_test, qid_train, qid_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_samples(X, y, qid):\n",
    "    X_train1, X_test1, y_train1, y_test1, qid_train1, qid_test1 = split_samples(X, y, qid, 1)\n",
    "    X_train2, X_test2, y_train2, y_test2, qid_train2, qid_test2 = split_samples(X, y, qid, 2)\n",
    "    X_train3, X_test3, y_train3, y_test3, qid_train3, qid_test3 = split_samples(X, y, qid, 3)\n",
    "    X_train = np.concatenate((X_train1, X_train2, X_train3), axis=0)\n",
    "    X_test = np.concatenate((X_test1, X_test2, X_test3), axis = 0)\n",
    "    y_train = np.concatenate((y_train1, y_train2, y_train3), axis=0)\n",
    "    y_test = np.concatenate((y_test1, y_test2, y_test3), axis = 0)\n",
    "    qid_train = np.concatenate((qid_train1, qid_train2, qid_train3), axis=0)\n",
    "    qid_test = np.concatenate((qid_test1, qid_test2, qid_test3), axis = 0)\n",
    "    return X_train, X_test, y_train, y_test, qid_train, qid_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "Training set = validation set = 80 % of the samples.\n",
    "\n",
    "Test set = 20 % of the samples.\n",
    "\n",
    "Train and validate the model with the training set and test the model accuracy with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, qid_train, qid_test = concatenate_samples(X, y, qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the adaRank model on the train set: 0.8429070153897805\n",
      "Accuracy percentage for the adaRank model on the train set: 84.29070153897806 %\n",
      "Accuracy for the adaRank model on the test set: 0.6402343972585051\n",
      "Accuracy percentage for the adaRank model on the test set: 64.02343972585051 %\n"
     ]
    }
   ],
   "source": [
    "model = AdaRank(scorer=NDCGScorer_qid(K=5))\n",
    "model.fit(X_train, y_train, qid_train, X_train, y_train, qid_train)\n",
    "pred_test = model.predict(X_test)\n",
    "pred_train = model.predict(X_train)\n",
    "accuracy_test = NDCGScorer_qid(K=5)(y_test,pred_test,qid_test).mean()\n",
    "accuracy_train = NDCGScorer_qid(K=5)(y_train,pred_train,qid_train).mean()\n",
    "print(\"Accuracy for the adaRank model on the train set:\",accuracy_train)\n",
    "print(\"Accuracy percentage for the adaRank model on the train set:\",accuracy_train*100,\"%\")\n",
    "print(\"Accuracy for the adaRank model on the test set:\",accuracy_test)\n",
    "print(\"Accuracy percentage for the adaRank model on the test set:\",accuracy_test*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "Train and test subsets taken from experiment 1.\n",
    "\n",
    "Validation set = 20 % of the samples of the train set.\n",
    "\n",
    "New train set = 80 % of the samples of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, qid_train, qid_val = concatenate_samples(X_train, y_train, qid_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the adaRank model on the train set: 0.8265721553313243\n",
      "Accuracy percentage for the adaRank model on the train set: 82.65721553313243 %\n",
      "Accuracy for the adaRank model on the test set: 0.6424362411702581\n",
      "Accuracy percentage for the adaRank model on the test set: 64.24362411702582 %\n"
     ]
    }
   ],
   "source": [
    "model = AdaRank(scorer=NDCGScorer_qid(K=5))\n",
    "model.fit(X_train, y_train, qid_train, X_val, y_val, qid_val)\n",
    "pred_test = model.predict(X_test)\n",
    "#print(y_test)\n",
    "#print(pred_test)\n",
    "pred_train = model.predict(X_train)\n",
    "accuracy_test = NDCGScorer_qid(K=5)(y_test,pred_test,qid_test).mean()\n",
    "accuracy_train = NDCGScorer_qid(K=5)(y_train,pred_train,qid_train).mean()\n",
    "print(\"Accuracy for the adaRank model on the train set:\",accuracy_train)\n",
    "print(\"Accuracy percentage for the adaRank model on the train set:\",accuracy_train*100,\"%\")\n",
    "print(\"Accuracy for the adaRank model on the test set:\",accuracy_test)\n",
    "print(\"Accuracy percentage for the adaRank model on the test set:\",accuracy_test*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3\n",
    "Train and validate the model with the whole dataset and test the model also with the whole dataset. This experiment has been done in order to see how good the training has been on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the adaRank model on the train set: 0.7776633344605005\n",
      "Accuracy percentage for the adaRank model on the train set: 77.76633344605006 %\n"
     ]
    }
   ],
   "source": [
    "model = AdaRank(scorer=NDCGScorer_qid(K=5))\n",
    "model.fit(X, y, qid, X, y, qid)\n",
    "pred = model.predict(X)\n",
    "accuracy = NDCGScorer_qid(K=5)(y,pred,qid).mean()\n",
    "print(\"Accuracy for the adaRank model on the train set:\",accuracy)\n",
    "print(\"Accuracy percentage for the adaRank model on the train set:\",accuracy*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The best model is the one trained on experiment 2 because it has the best accuracy on the training and test set compared with the experiment 1. For new input data, if we want to predict the ranking of the documents we will predict that ranking using the model trained on experiment 2. Additionally, when we are making the prediction for the test set, in the code line where we do \"pred_test = model.predict(X_test)\" the function predict returns the relevance score of each document. We are given a relevance score for each document and with that we know what is the relevance ranking of the documents. If you want to see how good the model works on prediction, just uncomment the print lines on experiment 2 and compare visually the predicted relevance scores for each document and the real output or ground truth relevance score of each document (y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
