{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from operator import itemgetter\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the three pages sepparately\n",
    "path = \"loinc_dataset-v2.xlsx\"\n",
    "document = xlrd.open_workbook(path)\n",
    "query1 = document.sheet_by_index(0)\n",
    "query2 = document.sheet_by_index(1)\n",
    "query3 = document.sheet_by_index(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the loinc_num and the long_name in a dictionary of the three querys\n",
    "# Glucose in blood is taken into account\n",
    "data = {}\n",
    "rows_query1 = query1.nrows\n",
    "# String of the first query that is glucose in blood\n",
    "part_list = query1.cell_value(0,0).split(\":\")[1].split(\" \")[1:4]\n",
    "text_query1 = \" \".join(part_list).lower()\n",
    "data[text_query1] = {}\n",
    "loinc_num = query1.cell_value(2,0)\n",
    "long_name = query1.cell_value(2,1)\n",
    "data[text_query1][loinc_num] = []\n",
    "data[text_query1][long_name] = []\n",
    "for i in range(3, rows_query1):\n",
    "    number_loinc = query1.cell_value(i,0)\n",
    "    text_name = query1.cell_value(i,1)\n",
    "    data[text_query1][loinc_num].append(number_loinc)\n",
    "    data[text_query1][long_name].append(text_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilirubin in plasma\n",
    "rows_query2 = query2.nrows\n",
    "part_list = query2.cell_value(0,0).split(\":\")[1].split(\" \")[1:4]\n",
    "text_query2 = \" \".join(part_list).lower()\n",
    "data[text_query2] = {}\n",
    "loinc_num = query2.cell_value(2,0)\n",
    "long_name = query2.cell_value(2,1)\n",
    "data[text_query2][loinc_num] = []\n",
    "data[text_query2][long_name] = []\n",
    "for i in range(3, rows_query2):\n",
    "    number_loinc = query2.cell_value(i,0)\n",
    "    text_name = query2.cell_value(i,1)\n",
    "    data[text_query2][loinc_num].append(number_loinc)\n",
    "    data[text_query2][long_name].append(text_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# White blood cells count\n",
    "rows_query3 = query3.nrows\n",
    "part_list = query3.cell_value(0,0).split(\":\")[1].split(\" \")[1:4]\n",
    "text_query3 = \" \".join(part_list).lower()\n",
    "data[text_query3] = {}\n",
    "loinc_num = query3.cell_value(2,0)\n",
    "long_name = query3.cell_value(2,1)\n",
    "data[text_query3][loinc_num] = []\n",
    "data[text_query3][long_name] = []\n",
    "for i in range(3, rows_query3):\n",
    "    number_loinc = query3.cell_value(i,0)\n",
    "    text_name = query3.cell_value(i,1)\n",
    "    data[text_query3][loinc_num].append(number_loinc)\n",
    "    data[text_query3][long_name].append(text_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()# object allowing a stemmer to return words in root form\n",
    "    tokens = wordpunct_tokenize(doc) # tokenize docs \n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    "    final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(docs):\n",
    "    pdocs = [preprocess_document(doc) for doc in docs]\n",
    "    dictionary = corpora.Dictionary(pdocs)\n",
    "    dictionary.save('/tmp/vsm.dict')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bag of words-based representation for each long name in the list\n",
    "def docs2bows(corpus, dictionary):\n",
    "    docs = [preprocess_document(d) for d in corpus]\n",
    "    vectors = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize('/tmp/vsm_docs.mm', vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weghted counterparts\n",
    "def create_TF_IDF_model(corpus):\n",
    "    dictionary = create_dictionary(corpus)\n",
    "    docs2bows(corpus, dictionary)\n",
    "    loaded_corpus = corpora.MmCorpus('/tmp/vsm_docs.mm')\n",
    "    tfidf = models.TfidfModel(loaded_corpus)\n",
    "    return tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_query(corpus, q, filename='/tmp/vsm_docs.mm'):\n",
    "    sorted_scores = []\n",
    "    sorted_docs = []\n",
    "    tfidf, dictionary = create_TF_IDF_model(corpus)\n",
    "    loaded_corpus = corpora.MmCorpus(filename)\n",
    "    index = similarities.MatrixSimilarity(loaded_corpus, num_features=len(dictionary))\n",
    "    pq = preprocess_document(q)\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = tfidf[vq]\n",
    "    sim = index[qtfidf]\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "    for doc, score in ranking:\n",
    "        if score > 0.0:\n",
    "            sorted_scores.append(score)\n",
    "            sorted_docs.append(corpus[doc])\n",
    "            # print(\"[ Score = \" + \"%f\" % score + \" ] \" + corpus[doc]);\n",
    "    return sorted_scores, sorted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white blood cells\n"
     ]
    }
   ],
   "source": [
    "print(text_query3)\n",
    "sorted_scores, sorted_docs = launch_query(data[text_query3][long_name], text_query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(r\"query3.txt\",\"w+\")\n",
    "# position = 1\n",
    "# for i in range(len(sorted_docs)):\n",
    "    # row = str(sorted_docs[i]) + \"$\" + str(position) + \"$\" + str(sorted_scores[i]) + \"$\" + \"3\" + \"\\n\"\n",
    "    # file.write(row)\n",
    "    # position += 1\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset already created in the directory dataset\n",
    "from adarank import AdaRank\n",
    "from metric import NDCGScorer_qid\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "qid = []\n",
    "i = 1\n",
    "with open(\"train.txt\") as fp:\n",
    "    for cnt, line in enumerate(fp):\n",
    "        X.append(float(line.split(\"$\")[1]))\n",
    "        y.append(float(line.split(\"$\")[2]))\n",
    "        qid.append(float(line.split(\"$\")[3]))\n",
    "        i += 1\n",
    "X = np.array(X)\n",
    "X = X.reshape(X.shape[0],1)\n",
    "y = np.array(y)\n",
    "y = y.reshape(y.shape[0],1)\n",
    "qid = np.array(qid)\n",
    "qid = qid.reshape(qid.shape[0],1)\n",
    "X_train, X_test, y_train, y_test, qid_train, qid_test = train_test_split(X, y, qid, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.06312482 19.68837475 30.93887461  2.81262496 36.56412454 14.06312482\n",
      " 16.87574979 36.56412454 36.56412454 22.50099972 33.75149957 56.25249929\n",
      " 75.94087404 53.43987432 28.12624964]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid, qid_train, qid_valid = train_test_split(X_train, y_train, qid_train, test_size=0.2, random_state=42)\n",
    "model = AdaRank(scorer=NDCGScorer_qid(K=5))\n",
    "model.fit(X, y.ravel(), qid.ravel(), X_valid, y_valid.ravel(), qid_valid.ravel())\n",
    "pred = model.predict(X_test)\n",
    "print(pred)\n",
    "print(NDCGScorer_qid(K=5)(y_test,pred,qid_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
